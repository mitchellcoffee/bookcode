{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items()\n",
    "        )\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, '[UNK]') for i in int_sequence\n",
    "        )\n",
    "    \n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"i write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\"\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)\n",
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization层\n",
    "# 文本标准化：转换为小写字母并删除标点符号\n",
    "# token化：利用空格进行拆分\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode='int')\n",
    "text_vectorization.adapt(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "print(vocabulary)\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir /category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 构建Dataset对象\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory('aclImdb/val', batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory('aclImdb/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"I have read the novel Reaper of Ben Mezrich a fews years ago and last night I accidentally came to see this adaption.<br /><br />Although it's been years since I read the story the first time, the differences between the novel and the movie are humongous. Very important elements, which made the whole thing plausible are just written out or changed to bad.<br /><br />If the plot sounds interesting to you: go and get the novel. Its much, much, much better.<br /><br />Still 4 out of 10 since it was hard to stop watching because of the great basic plot by Ben Mezrich.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print('inputs.shape:', inputs.shape)\n",
    "    print('inputs.dtype:', inputs.dtype)\n",
    "    print('targets.shape:', targets.shape)\n",
    "    print('targets.dtype:', targets.dtype)\n",
    "    print('inputs[0]:', inputs[0])\n",
    "    print('targets[0]:', targets[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个单词（一元语法）\n",
    "# 20000适用于文本分类的合适的词表大小\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode='multi_hot',\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print('inputs.shape:', inputs.shape)\n",
    "    print('inputs.dtype:', inputs.dtype)\n",
    "    print('targets.shape:', targets.shape)\n",
    "    print('targets.dtype:', targets.dtype)\n",
    "    print('inputs[0]:', inputs[0])\n",
    "    print('targets[0]:', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_token=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_token, ))\n",
    "    x = layers.Dense(hidden_dim, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.3885 - accuracy: 0.8394 - val_loss: 0.2976 - val_accuracy: 0.8816\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2669 - accuracy: 0.9025 - val_loss: 0.3014 - val_accuracy: 0.8882\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2376 - accuracy: 0.9181 - val_loss: 0.3205 - val_accuracy: 0.8908\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2291 - accuracy: 0.9229 - val_loss: 0.3498 - val_accuracy: 0.8890\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2233 - accuracy: 0.9276 - val_loss: 0.3543 - val_accuracy: 0.8896\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9305 - val_loss: 0.3725 - val_accuracy: 0.8862\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2149 - accuracy: 0.9334 - val_loss: 0.3861 - val_accuracy: 0.8868\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2062 - accuracy: 0.9319 - val_loss: 0.3984 - val_accuracy: 0.8878\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2066 - accuracy: 0.9375 - val_loss: 0.3983 - val_accuracy: 0.8890\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2108 - accuracy: 0.9372 - val_loss: 0.4134 - val_accuracy: 0.8850\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.2936 - accuracy: 0.8840\n",
      "Test acc: 0.884\n"
     ]
    }
   ],
   "source": [
    "# 对一元语法二进制模型进行训练和测试\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only=True)\n",
    "]\n",
    "# .cache()缓存到内存中，可以复用预处理的结果\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model('binary_1gram.keras')\n",
    "print(f'Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3814 - accuracy: 0.8392 - val_loss: 0.2911 - val_accuracy: 0.8886\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2565 - accuracy: 0.9115 - val_loss: 0.2941 - val_accuracy: 0.8908\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9288 - val_loss: 0.3076 - val_accuracy: 0.8960\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9362 - val_loss: 0.3228 - val_accuracy: 0.8974\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9427 - val_loss: 0.3389 - val_accuracy: 0.8932\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9452 - val_loss: 0.3585 - val_accuracy: 0.8948\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9480 - val_loss: 0.3539 - val_accuracy: 0.8964\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.9517 - val_loss: 0.3625 - val_accuracy: 0.8944\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9496 - val_loss: 0.3759 - val_accuracy: 0.8954\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.9522 - val_loss: 0.3956 - val_accuracy: 0.8872\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2814 - accuracy: 0.8914\n",
      "Test acc: 0.891\n"
     ]
    }
   ],
   "source": [
    "# 二元语法\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='multi_hot'\n",
    ")\n",
    "# 对二元语法二进制模型进行训练和测试\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_2gram.keras', save_best_only=True)\n",
    "]\n",
    "# .cache()缓存到内存中，可以复用预处理的结果\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model('binary_2gram.keras')\n",
    "print(f'Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.5498 - accuracy: 0.7235 - val_loss: 0.3886 - val_accuracy: 0.8468\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4025 - accuracy: 0.8019 - val_loss: 0.3388 - val_accuracy: 0.8504\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3687 - accuracy: 0.8239 - val_loss: 0.3432 - val_accuracy: 0.8586\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8464 - val_loss: 0.3194 - val_accuracy: 0.8886\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3151 - accuracy: 0.8577 - val_loss: 0.3288 - val_accuracy: 0.8758\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3036 - accuracy: 0.8666 - val_loss: 0.3384 - val_accuracy: 0.8606\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2905 - accuracy: 0.8680 - val_loss: 0.3251 - val_accuracy: 0.8782\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2730 - accuracy: 0.8733 - val_loss: 0.3507 - val_accuracy: 0.8602\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2690 - accuracy: 0.8751 - val_loss: 0.3428 - val_accuracy: 0.8766\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2678 - accuracy: 0.8774 - val_loss: 0.3439 - val_accuracy: 0.8806\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3110 - accuracy: 0.8883\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "# 二元语法的TF-IDF编码\n",
    "text_vectorization = TextVectorization(     \n",
    "    ngrams=2,     \n",
    "    max_tokens=20000,     \n",
    "    output_mode=\"tf_idf\" \n",
    ")\n",
    "# 训练和测试\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('tfidf_2gram.keras', save_best_only=True)\n",
    "]\n",
    "# .cache()缓存到内存中，可以复用预处理的结果\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model('tfidf_2gram.keras')\n",
    "print(f'Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.25 percent positive\n"
     ]
    }
   ],
   "source": [
    "# 模型包含TextVectorization层\n",
    "inputs = keras.Input(shape=(1,), dtype='string')\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)\n",
    "\n",
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备序列模型数据集\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建于one-hot编码的向量序列之上的序列模型\n",
    "# 双向LSTM + 分类层\n",
    "import tensorflow as tf\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "# 编码为20000维的二进制向量\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 62s 91ms/step - loss: 0.5121 - accuracy: 0.7653 - val_loss: 0.3372 - val_accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 57s 90ms/step - loss: 0.3289 - accuracy: 0.8849 - val_loss: 0.3144 - val_accuracy: 0.8740\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.2730 - accuracy: 0.8997 - val_loss: 0.3385 - val_accuracy: 0.8564\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 56s 89ms/step - loss: 0.2237 - accuracy: 0.9240 - val_loss: 0.6559 - val_accuracy: 0.7652\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.2016 - accuracy: 0.9319 - val_loss: 0.3382 - val_accuracy: 0.8856\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.2018 - accuracy: 0.9308 - val_loss: 0.3051 - val_accuracy: 0.8856\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.1619 - accuracy: 0.9492 - val_loss: 0.4405 - val_accuracy: 0.8454\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.1523 - accuracy: 0.9502 - val_loss: 0.3609 - val_accuracy: 0.8814\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.1326 - accuracy: 0.9584 - val_loss: 0.3403 - val_accuracy: 0.8818\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.1290 - accuracy: 0.9594 - val_loss: 0.3437 - val_accuracy: 0.8800\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3461 - accuracy: 0.8675\n",
      "Test acc: 0.867\n"
     ]
    }
   ],
   "source": [
    "# 训练一个简单的序列模型\n",
    "callbacks = [     \n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",                                    \n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds,\n",
    "          validation_data=int_val_ds, \n",
    "          epochs=10,           \n",
    "          callbacks=callbacks) \n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "# 速度非常慢，因为输入量很大，每个样本均为(600, 20000)\n",
    "# 测试精度也很低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 34s 49ms/step - loss: 0.4139 - accuracy: 0.8127 - val_loss: 0.3117 - val_accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.2432 - accuracy: 0.9053 - val_loss: 0.2793 - val_accuracy: 0.8904\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.1685 - accuracy: 0.9377 - val_loss: 0.3011 - val_accuracy: 0.8830\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.1291 - accuracy: 0.9557 - val_loss: 0.3390 - val_accuracy: 0.8792\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.0924 - accuracy: 0.9682 - val_loss: 0.3692 - val_accuracy: 0.8678\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.0667 - accuracy: 0.9777 - val_loss: 0.4472 - val_accuracy: 0.8630\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.0483 - accuracy: 0.9837 - val_loss: 0.4749 - val_accuracy: 0.8654\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 0.5169 - val_accuracy: 0.8580\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0249 - accuracy: 0.9922 - val_loss: 0.5590 - val_accuracy: 0.8654\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.5823 - val_accuracy: 0.8562\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.2945 - accuracy: 0.8800\n",
      "Text acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "# 从头楷书训练一个使用Embedding层的模型\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('embeddings_bidir_gru.keras', save_best_only=True)    \n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model('embeddings_bidir_gru.keras')\n",
    "print(f\"Text acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 解析GloVe词嵌入文件\n",
    "import numpy as np\n",
    "path_to_glove_file = 'glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(f'found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个可以加载到Embedding层中的嵌入模型\n",
    "embedding_dim = 100\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "# 单词到其词表索引的映射\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "# 准备一个矩阵，后续用GloVe向量填充\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    # 用索引为i的单词的词向量填充矩阵中的第i个元素\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# 使用Constant初始化方法在Embedding层中加载预训练词嵌入\n",
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 100)         2000000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               34048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,034,113\n",
      "Trainable params: 34,113\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 34s 49ms/step - loss: 0.5886 - accuracy: 0.6830 - val_loss: 0.4632 - val_accuracy: 0.7886\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.4664 - accuracy: 0.7843 - val_loss: 0.4221 - val_accuracy: 0.8126\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.4115 - accuracy: 0.8156 - val_loss: 0.3939 - val_accuracy: 0.8306\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.3725 - accuracy: 0.8396 - val_loss: 0.3637 - val_accuracy: 0.8422\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.3487 - accuracy: 0.8511 - val_loss: 0.3446 - val_accuracy: 0.8544\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3250 - accuracy: 0.8628 - val_loss: 0.3592 - val_accuracy: 0.8382\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3082 - accuracy: 0.8728 - val_loss: 0.3540 - val_accuracy: 0.8450\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.2938 - accuracy: 0.8789 - val_loss: 0.3180 - val_accuracy: 0.8680\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.2786 - accuracy: 0.8860 - val_loss: 0.3178 - val_accuracy: 0.8722\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.2664 - accuracy: 0.8913 - val_loss: 0.3225 - val_accuracy: 0.8738\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.2967 - accuracy: 0.8722\n",
      "Test acc: 0.872\n"
     ]
    }
   ],
   "source": [
    "# 使用与训练Embedding层的模型\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x) \n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs) \n",
    "model.compile(optimizer=\"rmsprop\",               \n",
    "              loss=\"binary_crossentropy\",               \n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary() \n",
    "callbacks = [     \n",
    "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",                                    \n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,           \n",
    "          callbacks=callbacks) \n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer编码器\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 输入词元向量的尺寸\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim,)]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # 增加掩码维数\n",
    "        if mask is not None:\n",
    "            mask = mask[: tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    # 实现序列化，以便保存模型\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_9 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_9 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 将Transform编码器用于文本分类\n",
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "# 将每个序列转换为单个向量\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 14s 18ms/step - loss: 0.4872 - accuracy: 0.7745 - val_loss: 0.3817 - val_accuracy: 0.8436\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.3115 - accuracy: 0.8701 - val_loss: 0.2937 - val_accuracy: 0.8776\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.2459 - accuracy: 0.9002 - val_loss: 0.2731 - val_accuracy: 0.8908\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.1938 - accuracy: 0.9258 - val_loss: 0.2902 - val_accuracy: 0.8876\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.1568 - accuracy: 0.9409 - val_loss: 0.3205 - val_accuracy: 0.8890\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.1320 - accuracy: 0.9499 - val_loss: 0.3921 - val_accuracy: 0.8804\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.1145 - accuracy: 0.9579 - val_loss: 0.3691 - val_accuracy: 0.8736\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.1002 - accuracy: 0.9625 - val_loss: 0.5053 - val_accuracy: 0.8524\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0888 - accuracy: 0.9663 - val_loss: 0.7077 - val_accuracy: 0.8326\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0775 - accuracy: 0.9711 - val_loss: 0.5518 - val_accuracy: 0.8496\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0689 - accuracy: 0.9756 - val_loss: 0.6640 - val_accuracy: 0.8670\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0589 - accuracy: 0.9780 - val_loss: 0.7867 - val_accuracy: 0.8386\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0539 - accuracy: 0.9814 - val_loss: 0.6604 - val_accuracy: 0.8608\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0440 - accuracy: 0.9847 - val_loss: 0.6091 - val_accuracy: 0.8654\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0424 - accuracy: 0.9862 - val_loss: 0.6259 - val_accuracy: 0.8566\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0326 - accuracy: 0.9886 - val_loss: 0.9249 - val_accuracy: 0.8410\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 0.5472 - val_accuracy: 0.8598\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0269 - accuracy: 0.9920 - val_loss: 0.7001 - val_accuracy: 0.8648\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0241 - accuracy: 0.9936 - val_loss: 0.7250 - val_accuracy: 0.8552\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.8103 - val_accuracy: 0.8452\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.2813 - accuracy: 0.8828\n",
      "Test acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "# 训练并评估基于Transformer编码器的模型\n",
    "callbacks = [     \n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",                                     \n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, \n",
    "          validation_data=int_val_ds, \n",
    "          epochs=20,           \n",
    "          callbacks=callbacks) \n",
    "model = keras.models.load_model(     \n",
    "    \"transformer_encoder.keras\",     \n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder}) \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置嵌入    \n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    # 生成掩码，从而可以忽略输入中填充的0\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_3 (Pos  (None, None, 256)        5273600   \n",
      " itionalEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_encoder_13 (Tra  (None, None, 256)        543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_5 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 文本分类Transformer\n",
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "# 将每个序列转换为单个向量\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [     \n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",                                     \n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, \n",
    "          validation_data=int_val_ds, \n",
    "          epochs=20,           \n",
    "          callbacks=callbacks) \n",
    "model = keras.models.load_model(     \n",
    "    \"full_transformer_encoder.keras\",\n",
    "    # 加载过程中提供自定义的类     \n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "                    \"PositionalEmbedding\": PositionalEmbedding}) \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器翻译任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"What if things don't work out?\", '[start] ¿Qué pasa si no lo conseguimos? [end]')\n"
     ]
    }
   ],
   "source": [
    "text_file = 'spa-eng/spa.txt'\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split('\\t')\n",
    "    spanish = '[start] ' + spanish + ' [end]'\n",
    "    text_pairs.append((english, spanish))\n",
    "\n",
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集、验证集、测试集\n",
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "# 保留[和]，去掉¿\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
    "    )\n",
    "\n",
    "# 15000个最常见的单词，句子长度限制为20个单词\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "# 英语层\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "# 西班牙语层\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备翻译任务的数据集\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1]\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f'targets.shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, ), dtype='int64')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU编码器\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype='int64', name='english')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "# 编码后的源句子即为双向GRU的最后一个输出\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode='sum'\n",
    ")(x)\n",
    "\n",
    "# GRU解码器\n",
    "past_target = keras.Input(shape=(None,), dtype='int64', name='spanish')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# 预测下一个词元\n",
    "target_next_step = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "# 端到端模型：源句子 和 目标句子 映射为 便宜一个时间步的目标句子\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 43s 26ms/step - loss: 1.6396 - accuracy: 0.4148 - val_loss: 1.3275 - val_accuracy: 0.5024\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 32s 24ms/step - loss: 1.3188 - accuracy: 0.5245 - val_loss: 1.1656 - val_accuracy: 0.5643\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 1.1746 - accuracy: 0.5755 - val_loss: 1.0792 - val_accuracy: 0.5983\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 1.0823 - accuracy: 0.6083 - val_loss: 1.0430 - val_accuracy: 0.6165\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 32s 24ms/step - loss: 1.0354 - accuracy: 0.6327 - val_loss: 1.0268 - val_accuracy: 0.6284\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 1.0037 - accuracy: 0.6514 - val_loss: 1.0220 - val_accuracy: 0.6345\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9840 - accuracy: 0.6653 - val_loss: 1.0238 - val_accuracy: 0.6369\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 31s 23ms/step - loss: 0.9702 - accuracy: 0.6751 - val_loss: 1.0260 - val_accuracy: 0.6394\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9599 - accuracy: 0.6831 - val_loss: 1.0271 - val_accuracy: 0.6413\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9516 - accuracy: 0.6895 - val_loss: 1.0308 - val_accuracy: 0.6412\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9456 - accuracy: 0.6943 - val_loss: 1.0328 - val_accuracy: 0.6433\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9420 - accuracy: 0.6974 - val_loss: 1.0369 - val_accuracy: 0.6431\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9394 - accuracy: 0.6993 - val_loss: 1.0370 - val_accuracy: 0.6443\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 31s 24ms/step - loss: 0.9374 - accuracy: 0.7013 - val_loss: 1.0411 - val_accuracy: 0.6430\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 32s 24ms/step - loss: 0.9372 - accuracy: 0.7018 - val_loss: 1.0428 - val_accuracy: 0.6435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ec8eee1300>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "seq2seq_rnn.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "He can scarcely write his name.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] Él puede haber [UNK] su nombre [end]\n",
      "-\n",
      "All the girls helped each other.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] todos los se nos la gente se una vez [end]\n",
      "-\n",
      "Tom asked Mary for a loan.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] tom pidió a mary que le una [UNK] [end]\n",
      "-\n",
      "We don't like this house.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] no nos gusta esta casa [end]\n",
      "-\n",
      "We're professors.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "[start] somos [UNK] [end]\n",
      "-\n",
      "Why do the five yen coin and the fifty yen coin have holes in the center?\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[start] por qué las cinco de los y el cinco años se tiene el que el las que en lo [UNK]\n",
      "-\n",
      "What's your favorite alcoholic beverage?\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] cuál es tu canción favorita [end]\n",
      "-\n",
      "Try this one.\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[start] [UNK] este [end]\n",
      "-\n",
      "This is the worst book I've ever read.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] este es el libro que he [UNK] hasta leer [end]\n",
      "-\n",
      "His dream has finally come true.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] su sueño se ha hecho realidad [end]\n",
      "-\n",
      "I don't even want to know.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] ni siquiera quiero saber [end]\n",
      "-\n",
      "We start classes next Monday.\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] a las escuela de el fin de semana el lunes [end]\n",
      "-\n",
      "Tom is creepy.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[start] tom es [UNK] [end]\n",
      "-\n",
      "A fool that considers himself smart is worse than any other fool.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] un [UNK] se parece que los [UNK] es más parece que es un buen profesor [end]\n",
      "-\n",
      "Elvis Presley is one of the most famous singers.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[start] [UNK] [UNK] es uno de los más [UNK] de una gran parte [end]\n",
      "-\n",
      "I have a big problem.\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[start] tengo un gran problema [end]\n",
      "-\n",
      "It cost me 10 dollars.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[start] me tomó diez dólares [end]\n",
      "-\n",
      "This heat is unbearable.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[start] este calor es [UNK] [end]\n",
      "-\n",
      "The story reminded me of my father.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] la historia me ha dicho a mi padre [end]\n",
      "-\n",
      "He has no wife, no children and no friends.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[start] Él no tiene ni a mary no y amigos no [end]\n"
     ]
    }
   ],
   "source": [
    "# 翻译新句子\n",
    "import numpy as np\n",
    "\n",
    "# 准备一个字典，将词元索引预测值映射为字符串词元\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "\n",
    "max_decoded_sentence_length = 20\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = '[start]'\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence]\n",
    "        )\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == '[end]':\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print('-')\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):         \n",
    "        config = super().get_config()         \n",
    "        config.update({             \n",
    "            \"embed_dim\": self.embed_dim,             \n",
    "            \"num_heads\": self.num_heads,             \n",
    "            \"dense_dim\": self.dense_dim,         \n",
    "        }) \n",
    "        return config\n",
    "    \n",
    "    # 生成因果掩码\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        # 生成形状为(sequence_length,  sequence_length)的矩阵，其中一半为1，另一半为0\n",
    "        mask = tf.cast(i >= j, dtype='int32')\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "            tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        # 因果掩码\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            # 输入掩码\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
    "            # 将两个掩码合并\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask\n",
    "        )\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " spanish (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_5 (Positi  (None, None, 256)   3845120     ['english[0][0]']                \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " positional_embedding_6 (Positi  (None, None, 256)   3845120     ['spanish[0][0]']                \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder_15 (Transf  (None, None, 256)   3155456     ['positional_embedding_5[0][0]'] \n",
      " ormerEncoder)                                                                                    \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   5259520     ['positional_embedding_6[0][0]', \n",
      " erDecoder)                                                       'transformer_encoder_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, None, 15000)  3855000     ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 端到端Transformer\n",
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='english')\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='spanish')\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(     \n",
    "    optimizer=\"rmsprop\",     \n",
    "    loss=\"sparse_categorical_crossentropy\",    \n",
    "      metrics=[\"accuracy\"]) \n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用transformer模型翻译新句子\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):     \n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])    \n",
    "    decoded_sentence = \"[start]\" \n",
    "    for i in range(max_decoded_sentence_length):         \n",
    "        tokenized_target_sentence = target_vectorization(            \n",
    "            [decoded_sentence])[:, :-1]  \n",
    "        # 对下一个词元进行采样       \n",
    "        predictions = transformer(             \n",
    "            [tokenized_input_sentence, tokenized_target_sentence])        \n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])        \n",
    "        sampled_token = spa_index_lookup[sampled_token_index]        \n",
    "        decoded_sentence += \" \" + sampled_token          \n",
    "        if sampled_token == \"[end]\": \n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs] \n",
    "for _ in range(20):     \n",
    "    input_sentence = random.choice(test_eng_texts)  \n",
    "    print(\"-\")  \n",
    "    print(input_sentence)  \n",
    "    print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
